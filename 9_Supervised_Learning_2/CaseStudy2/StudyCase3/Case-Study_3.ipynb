{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "71133460-dbba-4557-80e5-15e4334d322b",
   "metadata": {},
   "source": [
    "\n",
    "# Module 9: Supervised Learning II\n",
    "## Case Study – 3\n",
    "\n",
    "### Objective: \n",
    " * Employ SVM from scikit learn for binary classification. \n",
    " * Impact of preprocessing data and hyper parameter search using grid search. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "97849338-d0ec-4568-9641-58bc9ecc6cec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 777 entries, 0 to 776\n",
      "Data columns (total 18 columns):\n",
      " #   Column       Non-Null Count  Dtype  \n",
      "---  ------       --------------  -----  \n",
      " 0   Private      777 non-null    object \n",
      " 1   Apps         777 non-null    int64  \n",
      " 2   Accept       777 non-null    int64  \n",
      " 3   Enroll       777 non-null    int64  \n",
      " 4   Top10perc    777 non-null    int64  \n",
      " 5   Top25perc    777 non-null    int64  \n",
      " 6   F.Undergrad  777 non-null    int64  \n",
      " 7   P.Undergrad  777 non-null    int64  \n",
      " 8   Outstate     777 non-null    int64  \n",
      " 9   Room.Board   777 non-null    int64  \n",
      " 10  Books        777 non-null    int64  \n",
      " 11  Personal     777 non-null    int64  \n",
      " 12  PhD          777 non-null    int64  \n",
      " 13  Terminal     777 non-null    int64  \n",
      " 14  S.F.Ratio    777 non-null    float64\n",
      " 15  perc.alumni  777 non-null    int64  \n",
      " 16  Expend       777 non-null    int64  \n",
      " 17  Grad.Rate    777 non-null    int64  \n",
      "dtypes: float64(1), int64(16), object(1)\n",
      "memory usage: 109.4+ KB\n",
      "None\n",
      "Private        0\n",
      "Apps           0\n",
      "Accept         0\n",
      "Enroll         0\n",
      "Top10perc      0\n",
      "Top25perc      0\n",
      "F.Undergrad    0\n",
      "P.Undergrad    0\n",
      "Outstate       0\n",
      "Room.Board     0\n",
      "Books          0\n",
      "Personal       0\n",
      "PhD            0\n",
      "Terminal       0\n",
      "S.F.Ratio      0\n",
      "perc.alumni    0\n",
      "Expend         0\n",
      "Grad.Rate      0\n",
      "dtype: int64\n",
      "  Private  Apps  Accept  Enroll  Top10perc  Top25perc  F.Undergrad  \\\n",
      "0     Yes  1660    1232     721         23         52         2885   \n",
      "1     Yes  2186    1924     512         16         29         2683   \n",
      "2     Yes  1428    1097     336         22         50         1036   \n",
      "3     Yes   417     349     137         60         89          510   \n",
      "4     Yes   193     146      55         16         44          249   \n",
      "\n",
      "   P.Undergrad  Outstate  Room.Board  Books  Personal  PhD  Terminal  \\\n",
      "0          537      7440        3300    450      2200   70        78   \n",
      "1         1227     12280        6450    750      1500   29        30   \n",
      "2           99     11250        3750    400      1165   53        66   \n",
      "3           63     12960        5450    450       875   92        97   \n",
      "4          869      7560        4120    800      1500   76        72   \n",
      "\n",
      "   S.F.Ratio  perc.alumni  Expend  Grad.Rate  \n",
      "0       18.1           12    7041         60  \n",
      "1       12.2           16   10527         56  \n",
      "2       12.9           30    8735         54  \n",
      "3        7.7           37   19016         59  \n",
      "4       11.9            2   10922         15  \n",
      "               Apps        Accept       Enroll   Top10perc   Top25perc  \\\n",
      "count    777.000000    777.000000   777.000000  777.000000  777.000000   \n",
      "mean    3001.638353   2018.804376   779.972973   27.558559   55.796654   \n",
      "std     3870.201484   2451.113971   929.176190   17.640364   19.804778   \n",
      "min       81.000000     72.000000    35.000000    1.000000    9.000000   \n",
      "25%      776.000000    604.000000   242.000000   15.000000   41.000000   \n",
      "50%     1558.000000   1110.000000   434.000000   23.000000   54.000000   \n",
      "75%     3624.000000   2424.000000   902.000000   35.000000   69.000000   \n",
      "max    48094.000000  26330.000000  6392.000000   96.000000  100.000000   \n",
      "\n",
      "        F.Undergrad   P.Undergrad      Outstate   Room.Board        Books  \\\n",
      "count    777.000000    777.000000    777.000000   777.000000   777.000000   \n",
      "mean    3699.907336    855.298584  10440.669241  4357.526384   549.380952   \n",
      "std     4850.420531   1522.431887   4023.016484  1096.696416   165.105360   \n",
      "min      139.000000      1.000000   2340.000000  1780.000000    96.000000   \n",
      "25%      992.000000     95.000000   7320.000000  3597.000000   470.000000   \n",
      "50%     1707.000000    353.000000   9990.000000  4200.000000   500.000000   \n",
      "75%     4005.000000    967.000000  12925.000000  5050.000000   600.000000   \n",
      "max    31643.000000  21836.000000  21700.000000  8124.000000  2340.000000   \n",
      "\n",
      "          Personal         PhD    Terminal   S.F.Ratio  perc.alumni  \\\n",
      "count   777.000000  777.000000  777.000000  777.000000   777.000000   \n",
      "mean   1340.642214   72.660232   79.702703   14.089704    22.743887   \n",
      "std     677.071454   16.328155   14.722359    3.958349    12.391801   \n",
      "min     250.000000    8.000000   24.000000    2.500000     0.000000   \n",
      "25%     850.000000   62.000000   71.000000   11.500000    13.000000   \n",
      "50%    1200.000000   75.000000   82.000000   13.600000    21.000000   \n",
      "75%    1700.000000   85.000000   92.000000   16.500000    31.000000   \n",
      "max    6800.000000  103.000000  100.000000   39.800000    64.000000   \n",
      "\n",
      "             Expend  Grad.Rate  \n",
      "count    777.000000  777.00000  \n",
      "mean    9660.171171   65.46332  \n",
      "std     5221.768440   17.17771  \n",
      "min     3186.000000   10.00000  \n",
      "25%     6751.000000   53.00000  \n",
      "50%     8377.000000   65.00000  \n",
      "75%    10830.000000   78.00000  \n",
      "max    56233.000000  118.00000  \n",
      "Private\n",
      "Yes    565\n",
      "No     212\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.svm import LinearSVC, SVC\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "\n",
    "\n",
    "# Step 1: Load and Explore the Data\n",
    "# We start by examining the dataset for missing values, data types, and general structure. \n",
    "# From the given description, no missing values are present.\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv('College.csv')\n",
    "\n",
    "# Check basic info\n",
    "print(data.info())\n",
    "print(data.isnull().sum())\n",
    "\n",
    "# Preview the data\n",
    "print(data.head())\n",
    "\n",
    "print(data.describe())\n",
    "\n",
    "# Count of each class in the target\n",
    "print(data['Private'].value_counts())\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "6ea04dd2-be23-4cf8-a32c-c1ccb3d4d28b",
   "metadata": {},
   "source": [
    "# Exploring the Data\n",
    "The dataset contains 777 observations with 18 attributes, including the target variable Private (binary: \"Yes\" for private colleges, \"No\" for public colleges).\n",
    "Features like Apps, Accept, Enroll, and Grad.Rate represent application and admission statistics, while Outstate, Room.Board, and Books are cost-related attributes.\n",
    "There is a moderate imbalance in the target variable (Yes: 565, No: 212).\n",
    "\n",
    "# Feature Analysis\n",
    "Features such as Apps, Accept, and Enroll represent the number of applicants and admissions, which directly correlate with a college’s status as private or public.\n",
    "Cost-related features like Outstate (tuition for out-of-state students) and Room.Board (housing costs) were expected to be highly discriminative since private colleges typically have higher costs.\n",
    "Academic features like Top10perc (percent of new students from the top 10% of their high school class) and Grad.Rate (graduation rate) provide additional insights into the quality of education.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "df8d028b-bf29-4c62-bdf0-0978f70a52cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Encode the Target Variable and Split the Data\n",
    "# Use LabelEncoder to convert the Private column into numerical form and split the data into training and testing sets.\n",
    "\n",
    "# Encode the target variable\n",
    "le = LabelEncoder()\n",
    "data['Private'] = le.fit_transform(data['Private'])  # 'Yes' -> 1, 'No' -> 0\n",
    "\n",
    "# Split the data into features and target\n",
    "X = data.drop(columns=['Private'])\n",
    "y = data['Private']\n",
    "\n",
    "\n",
    "'''\n",
    "# Oversampling the Minority Class\n",
    "#We can use oversampling techniques to generate synthetic samples for the minority class (\"No\").\n",
    "#Common methods include:\n",
    "#Random Oversampling: Duplicates random samples from the minority class.\n",
    "#SMOTE (Synthetic Minority Oversampling Technique): Generates synthetic samples by interpolating between existing minority class samples.\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# Apply SMOTE to balance the classes\n",
    "smote = SMOTE(random_state=42)\n",
    "X, y = smote.fit_resample(X, y)\n",
    "'''\n",
    "\n",
    "'''\n",
    "# Undersampling the Majority Class\n",
    "#Undersampling reduces the number of samples in the majority class to match the minority class size. \n",
    "#While this can improve balance, it risks losing valuable information\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "# Apply undersampling\n",
    "undersampler = RandomUnderSampler(random_state=42)\n",
    "X, y = undersampler.fit_resample(X, y)\n",
    "'''\n",
    "\n",
    "\n",
    "# Split into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c996c03c-89bf-4879-b720-dd867f9c84ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-Validation Accuracy (Linear SVM, no scaling): 93.44%\n",
      "Test Accuracy: 92.95%\n",
      "Precision: 0.95, Recall: 0.94, F1-Score: 0.95\n",
      "Confusion Matrix:\n",
      "[[ 42   5]\n",
      " [  6 103]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.89      0.88        47\n",
      "           1       0.95      0.94      0.95       109\n",
      "\n",
      "    accuracy                           0.93       156\n",
      "   macro avg       0.91      0.92      0.92       156\n",
      "weighted avg       0.93      0.93      0.93       156\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Step 3: Fit a Linear SVM and Observe Accuracy\n",
    "# We’ll fit a LinearSVC without any preprocessing to establish a baseline accuracy.\n",
    "\n",
    "# Fit LinearSVC\n",
    "linear_svc = LinearSVC(max_iter=10000, random_state=42)\n",
    "linear_svc.fit(X_train, y_train)\n",
    "\n",
    "# Predict and evaluate\n",
    "y_pred = linear_svc.predict(X_test)\n",
    "# Cross-validation accuracy\n",
    "cv_scores = cross_val_score(linear_svc, X, y, cv=5, scoring='accuracy')\n",
    "print(f\"Cross-Validation Accuracy (Linear SVM, no scaling): {cv_scores.mean() * 100:.2f}%\")\n",
    "\n",
    "# Evaluate metrics\n",
    "precision, recall, f1, _ = precision_recall_fscore_support(y_test, y_pred, average='binary')\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "print(f\"Test Accuracy: {accuracy_score(y_test, y_pred) * 100:.2f}%\")\n",
    "print(f\"Precision: {precision:.2f}, Recall: {recall:.2f}, F1-Score: {f1:.2f}\")\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "4b4e27c1-de11-4400-8d43-2f03e81d76b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-Validation Accuracy (Linear SVM, with scaling): 93.31%\n",
      "Test Accuracy (Scaled): 92.95%\n",
      "Precision: 0.95, Recall: 0.94, F1-Score: 0.95\n",
      "Confusion Matrix (Scaled):\n",
      "[[ 42   5]\n",
      " [  6 103]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.89      0.88        47\n",
      "           1       0.95      0.94      0.95       109\n",
      "\n",
      "    accuracy                           0.93       156\n",
      "   macro avg       0.91      0.92      0.92       156\n",
      "weighted avg       0.93      0.93      0.93       156\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Step 4: Preprocess the Data with StandardScaler\n",
    "# Standardize the features and fit the same LinearSVC model again to observe changes in accuracy.\n",
    "\n",
    "# Standardize the data\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Fit LinearSVC with scaled data\n",
    "linear_svc_scaled = LinearSVC(max_iter=10000, random_state=42)\n",
    "linear_svc_scaled.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Predict and evaluate\n",
    "y_pred_scaled = linear_svc_scaled.predict(X_test_scaled)\n",
    "# Cross-validation accuracy\n",
    "cv_scores_scaled = cross_val_score(linear_svc_scaled, X_scaled, y, cv=5, scoring='accuracy')\n",
    "print(f\"Cross-Validation Accuracy (Linear SVM, with scaling): {cv_scores_scaled.mean() * 100:.2f}%\")\n",
    "\n",
    "# Evaluate metrics\n",
    "precision_scaled, recall_scaled, f1_scaled, _ = precision_recall_fscore_support(y_test, y_pred_scaled, average='binary')\n",
    "conf_matrix_scaled = confusion_matrix(y_test, y_pred_scaled)\n",
    "\n",
    "print(f\"Test Accuracy (Scaled): {accuracy_score(y_test, y_pred_scaled) * 100:.2f}%\")\n",
    "print(f\"Precision: {precision_scaled:.2f}, Recall: {recall_scaled:.2f}, F1-Score: {f1_scaled:.2f}\")\n",
    "print(\"Confusion Matrix (Scaled):\")\n",
    "print(conf_matrix_scaled)\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred_scaled))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "4eac47f8-98c7-4b1f-bc43-87cabda70f21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'C': 100, 'gamma': 0.001, 'kernel': 'rbf'}\n",
      "Best Cross-Validation Score: 95.17%\n",
      "Cross-Validation Accuracy (Best Non-Linear SVM): 94.08%\n",
      "Test Accuracy (Best Non-Linear SVM): 92.31%\n",
      "Precision: 0.94, Recall: 0.94, F1-Score: 0.94\n",
      "Confusion Matrix (Best Non-Linear SVM):\n",
      "[[ 41   6]\n",
      " [  6 103]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.87      0.87        47\n",
      "           1       0.94      0.94      0.94       109\n",
      "\n",
      "    accuracy                           0.92       156\n",
      "   macro avg       0.91      0.91      0.91       156\n",
      "weighted avg       0.92      0.92      0.92       156\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Step 5: Hyperparameter Search for Non-Linear SVM\n",
    "# Perform a grid search to find the best hyperparameters for a non-linear SVM (e.g., RBF kernel).\n",
    "\n",
    "# Define the SVM model\n",
    "svm_model = SVC()\n",
    "\n",
    "# Define parameter grid\n",
    "param_grid = {\n",
    "    'C': [0.1, 1, 10, 100],\n",
    "    'gamma': [1, 0.1, 0.01, 0.001],\n",
    "    'kernel': ['rbf', 'poly']\n",
    "}\n",
    "\n",
    "# Perform grid search\n",
    "grid_search = GridSearchCV(svm_model, param_grid, cv=5, scoring='accuracy', verbose=0)\n",
    "grid_search.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Print best parameters and accuracy\n",
    "best_model = grid_search.best_estimator_\n",
    "print(f\"Best Parameters: {grid_search.best_params_}\")\n",
    "print(f\"Best Cross-Validation Score: {grid_search.best_score_ * 100:.2f}%\")\n",
    "\n",
    "# Evaluate the best model on the test data\n",
    "y_pred_best = best_model.predict(X_test_scaled)\n",
    "# Cross-validation accuracy\n",
    "cv_scores_best = cross_val_score(best_model, X_scaled, y, cv=5, scoring='accuracy')\n",
    "print(f\"Cross-Validation Accuracy (Best Non-Linear SVM): {cv_scores_best.mean() * 100:.2f}%\")\n",
    "\n",
    "# Evaluate metrics\n",
    "precision_best, recall_best, f1_best, _ = precision_recall_fscore_support(y_test, y_pred_best, average='binary')\n",
    "conf_matrix_best = confusion_matrix(y_test, y_pred_best)\n",
    "\n",
    "print(f\"Test Accuracy (Best Non-Linear SVM): {accuracy_score(y_test, y_pred_best) * 100:.2f}%\")\n",
    "print(f\"Precision: {precision_best:.2f}, Recall: {recall_best:.2f}, F1-Score: {f1_best:.2f}\")\n",
    "print(\"Confusion Matrix (Best Non-Linear SVM):\")\n",
    "print(conf_matrix_best)\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred_best))\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "6c302b6a-0350-4e41-a78b-1bf57d5b7524",
   "metadata": {},
   "source": [
    "\n",
    "# Summary and Conclusions:\n",
    "This assignment aimed to explore the use of Support Vector Machines (SVM) for binary classification using scikit-learn. It included investigating the impact of preprocessing and hyperparameter tuning on model performance.\n",
    "\n",
    "\n",
    "# Key Findings\n",
    "\n",
    "I Baseline Model: Linear SVM Without Scaling\n",
    "Cross-Validation Accuracy: 93.44%\n",
    "Test Accuracy: 92.95%\n",
    "The model performed well even without scaling, but scaling is generally recommended for SVMs due to their sensitivity to feature magnitudes.\n",
    "\n",
    "\n",
    "II Scaled Linear SVM\n",
    "Cross-Validation Accuracy: 93.31%\n",
    "Test Accuracy: 92.95%\n",
    "Precision: 0.95, Recall: 0.94, F1-Score: 0.95\n",
    "Scaling the data did not significantly change performance in this case, but it ensures robustness when handling features with large ranges.\n",
    "\n",
    "\n",
    "III Non-Linear SVM with Hyperparameter Tuning\n",
    "Using grid search, the best parameters were found to be:\n",
    "C: 100\n",
    "gamma: 0.001\n",
    "kernel: 'rbf'\n",
    "Cross-Validation Accuracy: 95.17%\n",
    "Test Accuracy: 92.31%\n",
    "While the accuracy and metrics were comparable to the linear model, the RBF kernel may provide more flexibility in capturing complex patterns in other datasets.\n",
    "\n",
    "\n",
    "Final Recommendations\n",
    "\n",
    "Scaling Matters: Although scaling did not significantly impact results here, it is generally crucial for SVMs, particularly for datasets with a wide range of feature magnitudes.\n",
    "\n",
    "Class Imbalance:\n",
    "The class imbalance was moderate and did not affect performance drastically.\n",
    "For more imbalanced datasets, methods like oversampling (e.g., SMOTE) or class-weight adjustments should be considered.\n",
    "\n",
    "Hyperparameter Tuning:\n",
    "Non-linear SVMs with hyperparameter tuning provided slightly better cross-validation accuracy, but the added computational cost may not justify its use unless needed for more complex datasets.\n",
    "\n",
    "Future Exploration:\n",
    "Incorporate other models like Random Forests or Gradient Boosted Trees to compare interpretability and performance.\n",
    "Investigate feature importance to identify the most discriminative attributes.\n",
    "\n",
    "\n",
    "This study demonstrated the robustness of SVMs for binary classification tasks, showing that careful preprocessing and parameter optimization can yield competitive results.\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
