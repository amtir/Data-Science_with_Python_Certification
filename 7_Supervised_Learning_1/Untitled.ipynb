{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "156d1f07-3bbc-4e15-988c-21834061bed2",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f43bad2-2d6f-4744-b7f9-995689444e4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tree based Algo  (Non-Linear)\n",
    "# Mostly for CLASSIFICATION but could be used for regression as well\n",
    "\n",
    "# 1 DT  Decision Tree\n",
    "# 2 RF  Random-Forest\n",
    "# XG Boost Boosting part \n",
    "\n",
    "# Important paramters , nodes features from the data\n",
    "# N of Hours\n",
    "# Distance from home\n",
    "# Payscale\n",
    "# Humidity, Weather, Wind\n",
    "\n",
    "# Final decision, leaf decision -->  YES or NO response \n",
    "\n",
    "# Humidity --> High ? Normal ? \n",
    "# Wind  --> Strong? \n",
    "\n",
    "# Pruning  --> keep only important meaningfult features nodes \n",
    "\n",
    "# Forest --> Random Forest  made of trees \n",
    "\n",
    "# Pure/Impure Node (features)\n",
    "# Pure ? Impure Node ? How to measure \n",
    "# ASM Attribute Selection Measures \n",
    "# IG Information Gain - Entropy change\n",
    "# Node with more importance \n",
    "# GI Gini impurity Index (lower index value is preferred )\n",
    "# A, B, C, D, E  \n",
    "# GI of A has the lowest GI (Most important) \n",
    "# A -> C-> B -> ...\n",
    "\n",
    "# Under Over Best fitting\n",
    "# Underfitting: TR-data 54%  & Test-Data: 40%\n",
    "# Overfitting: TR-Data: 98% (calibrated) & Test-Data: 58% (Not a good a model)\n",
    "# Bestfitting: TR-Data: 85% & Test-Data: 81%  (good model)\n",
    "# How to remove Overfitting?  delta more than 10% --> Overfitting\n",
    "# when working with a large data features, we'll get easily overfitted \n",
    "# Because high diemention -->  Dimension reduction \n",
    "# or Hyperparam Tuning \n",
    "# or Ensemble learning\n",
    "\n",
    "# Ensemble Learning \n",
    "\n",
    "# Weak ? Strong learners\n",
    "# Weak learner model \n",
    "# Strong \n",
    "\n",
    "# Models M1, M2, M3, M4  predict Cats or Dogs  -->  \n",
    "# Max Voting --> Final Predicton is CAT\n",
    "# Also possible for regression problem  --> instead of Voting we use AVG\n",
    "\n",
    "# Random-Forest \n",
    "# Ensemble learning --> rely on many/multiple decision trees \n",
    "# select subsets DS1, DS2, DS3, DS4, DS5\n",
    "# Models M1, M2, M3, M4, M5\n",
    "# Better results \n",
    "# Bagging approach // parallel approach,  Bootstrap-aggregation \n",
    "# Boosting (sequential approach) \n",
    "# scikitlearn\n",
    "\n",
    "# Hyper-parameters tunning \n",
    "# F(A,B,C) -> O/P  --> Best Model\n",
    "# serach\n",
    "# Random search \n",
    "# Grid-search -> best approach systematic approach \n",
    "# Gradient Descent \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
