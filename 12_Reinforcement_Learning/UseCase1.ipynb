{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "02bf969b-c50e-4576-a2c9-46cc67f8a775",
   "metadata": {},
   "source": [
    "# Module 12: Reinforcement Learning\n",
    "## Case Study – 1: Optimal Path for Logistics\n",
    " \n",
    "#### **Objective**  \n",
    "The goal is to optimize delivery paths using RL, minimizing fuel costs and delays. \n",
    "\n",
    "#### **Approach**  \n",
    "1. **Graph Representation**:  \n",
    "   - Delivery routes are represented as a **graph**:\n",
    "     - **Nodes**: Delivery points (e.g., warehouses, customer locations).  \n",
    "     - **Edges**: Routes between delivery points, weighted by **distances** (in km).  \n",
    "\n",
    "2. **Reward Matrix**:  \n",
    "   - The distances between nodes are converted into a **reward matrix (\\(R\\))**:  \n",
    "     R[i][j] = int(100/distance)   \n",
    "   - **Shorter distances yield higher rewards.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e3d6a3d-f4b7-4d78-8b59-30af8c897d61",
   "metadata": {},
   "source": [
    "<img src=\"cities_graph_distance.png\" alt=\"Alt Text\" width=\"300\" height=\"300\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2fdc0545-0939-4958-8c91-0867dc59dc1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained Q-Matrix:\n",
      "[[  0.   72.   85.   60.4   0.    0. ]\n",
      " [ 72.    0.   83.    0.   85.    0. ]\n",
      " [ 73.   71.    0.    0.  100.   74. ]\n",
      " [ 70.    0.    0.    0.    0.   73. ]\n",
      " [  0.   73.  100.    0.    0.   82. ]\n",
      " [  0.    0.   82.   59.4  90.    0. ]]\n",
      "Optimal Path with Constraints: [0, 2, 4, 5]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Define distances (in km) as per the provided graph\n",
    "distances = np.matrix([\n",
    "    [-1, 25, 20, 50, -1, -1],\n",
    "    [25, -1, 30, -1, 20, -1],\n",
    "    [20, 30, -1, -1, 5, 45],\n",
    "    [50, -1, -1, -1, -1, 70],\n",
    "    [-1, 20, 5, -1, -1, 10],\n",
    "    [-1, -1, 45, 70, 10, -1]\n",
    "])\n",
    "\n",
    "# Convert distances to rewards using the formula int(100 / distance)\n",
    "R = np.where(distances > 0, np.int32(100 / distances), -1)\n",
    "\n",
    "# Initialize Q-matrix\n",
    "Q = np.matrix(np.zeros(R.shape))\n",
    "\n",
    "# Parameters\n",
    "gamma = 0.8  # Discount factor\n",
    "alpha = 0.5  # Learning rate\n",
    "num_episodes = 10000  # Number of training iterations\n",
    "\n",
    "# Q-learning algorithm\n",
    "for episode in range(num_episodes):\n",
    "    state = np.random.randint(0, R.shape[0])  # Random initial state\n",
    "\n",
    "    while True:\n",
    "        # Get possible actions for the current state\n",
    "        actions = np.where(np.array(R[state]).flatten() >= 0)[0]\n",
    "\n",
    "        # Choose a random action from the possible actions\n",
    "        action = np.random.choice(actions, 1)[0]\n",
    "\n",
    "        # Calculate the maximum Q-value for the next state\n",
    "        next_state = action\n",
    "        max_next_q = np.max(Q[next_state,])\n",
    "\n",
    "        # Update Q-value using the Q-learning formula\n",
    "        Q[state, action] = Q[state, action] + alpha * (\n",
    "            R[state, action] + gamma * max_next_q - Q[state, action]\n",
    "        )\n",
    "\n",
    "        # Move to the next state\n",
    "        state = next_state\n",
    "\n",
    "        # End episode if goal state (state 5) is reached\n",
    "        if state == 5:\n",
    "            break\n",
    "\n",
    "# Normalize the Q-matrix\n",
    "Q_normalized = Q / np.max(Q) * 100\n",
    "\n",
    "# Display the trained Q-matrix\n",
    "print(\"Trained Q-Matrix:\")\n",
    "print(Q_normalized)\n",
    "\n",
    "\n",
    "\n",
    "def optimal_path_with_constraints(start_state):\n",
    "    steps = [start_state]\n",
    "    current_state = start_state\n",
    "    visited = set()  # Keep track of visited nodes\n",
    "\n",
    "    while current_state != 5:  # Goal state is 5\n",
    "        visited.add(current_state)  # Mark the current state as visited\n",
    "\n",
    "        # Find the next state with the highest Q-value that is not visited\n",
    "        q_values = Q[current_state].A1  # Flatten the Q-row\n",
    "        #print(q_values)\n",
    "        mask = np.array([i in visited for i in range(len(q_values))])  # Create a boolean mask\n",
    "        q_values[mask] = -np.inf  # Set visited nodes' Q-values to -inf to avoid revisiting\n",
    "        #print(q_values)\n",
    "\n",
    "        # Select the best unvisited state\n",
    "        next_state = np.argmax(q_values)\n",
    "        if next_state in visited or q_values[next_state] == -np.inf:\n",
    "            print(\"No valid path found. Stuck!\")\n",
    "            return steps  # Return the path so far if stuck\n",
    "\n",
    "        steps.append(int(next_state))\n",
    "        current_state = next_state\n",
    "\n",
    "    return steps\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Find the optimal path starting from node 0\n",
    "path = optimal_path_with_constraints(0)\n",
    "print(\"Optimal Path with Constraints:\", path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bf88e1b-f545-48d2-a9b2-66a78df9f52b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bb600c76-3cc1-4542-9dd9-5ef4967ce19d",
   "metadata": {},
   "source": [
    "# Case Study – 1: Optimal Path for Logistics\n",
    "\n",
    "## **Domain**: Logistics  \n",
    "## **Focus**: Optimal Path  \n",
    "\n",
    "### **Business Challenge/Requirement**  \n",
    "BluEx is a leading logistics company in India, known for its efficient delivery of packets to customers. However, van drivers are taking suboptimal delivery paths, resulting in delays and higher fuel costs.  \n",
    "As an ML expert, you are tasked with creating a Reinforcement Learning (RL) model to identify efficient delivery paths and reduce costs.\n",
    "\n",
    "---\n",
    "\n",
    "### **Key Issues**\n",
    "- Delivery routes have multiple attributes (distances, cost) to optimize.\n",
    "- Classification of routes could be tricky due to the dynamic nature of logistics.\n",
    "\n",
    "---\n",
    "\n",
    "### **Considerations**\n",
    "- Reinforcement Learning is complex; a sample flow is expected for this assignment.\n",
    "- Full-fledged implementation will be completed later by the team.\n",
    "- No data volume is provided; sample data is hardcoded.\n",
    "\n",
    "---\n",
    "\n",
    "### **Sample Solution**\n",
    "#### **Objective**  \n",
    "The goal is to optimize delivery paths using RL, minimizing fuel costs and delays. \n",
    "\n",
    "#### **Approach**  \n",
    "1. **Graph Representation**:  \n",
    "   - Delivery routes are represented as a **graph**:\n",
    "     - **Nodes**: Delivery points (e.g., warehouses, customer locations).  \n",
    "     - **Edges**: Routes between delivery points, weighted by **distances** (in km).  \n",
    "\n",
    "2. **Reward Matrix**:  \n",
    "   - The distances between nodes are converted into a **reward matrix (\\(R\\))**:  \n",
    "     R[i][j] = int(100/distance)   \n",
    "   - **Shorter distances yield higher rewards.**\n",
    "\n",
    "3. **Reinforcement Learning with Q-Learning**:  \n",
    "   - **Q-learning algorithm** is used to train the model:  \n",
    "     - The **Q-matrix** is updated iteratively to store the best cumulative rewards (lowest costs) for moving between nodes.\n",
    "\n",
    "4. **Training Process**:  \n",
    "   - The agent explores the graph for 10,000 iterations.  \n",
    "   - Each iteration improves the Q-matrix by learning which routes lead to better rewards.  \n",
    "\n",
    "5. **Extracting the Optimal Path**:  \n",
    "   - After training, the model identifies the **shortest path** by following the highest Q-values from the starting node to the goal node.  \n",
    "\n",
    "---\n",
    "\n",
    "### **Simplified Example**  \n",
    "- **Graph**:  \n",
    "  - Nodes: \\(0, 1, 2, 3, 4, 5\\) (represent delivery points).  \n",
    "  - Edges: Distances between nodes (e.g., \\(25\\) km from node \\(0\\) to \\(1\\)).  \n",
    "\n",
    "- **Sample Output**:  \n",
    "  - **Optimal Path**: From node \\(0\\) to node \\(5\\), the shortest path might be \\(0 → 2 → 4 → 5\\).  \n",
    "\n",
    "---\n",
    "\n",
    "### **Why RL is Relevant**  \n",
    "The sample demonstrates how RL solves BluEx's challenge by:  \n",
    "1. Modeling delivery points as nodes and distances as edges.  \n",
    "2. Using rewards to represent shorter distances and fuel savings.  \n",
    "3. Dynamically learning the best routes through Q-learning.\n",
    "\n",
    "This solution provides a foundational **sample flow** to address BluEx's logistics problem.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a14ff8e-c6cb-4e0e-a1aa-e30bbc7db7ce",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
