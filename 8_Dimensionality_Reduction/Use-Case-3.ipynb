{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "252d37d6-20f0-46a4-86ab-61e0a94362f1",
   "metadata": {},
   "source": [
    "# Module 8: Dimensionality Reduction\n",
    "## Case Study ‚Äì 3\n",
    "\n",
    "### Dimensionality Reduction and Supervised Learning for Breast Cancer Classification: A Comparative Study of PCA, LDA, and Ensemble Methods. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0b5a9667-c283-4de8-8a6d-c2fc769db208",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 569 entries, 0 to 568\n",
      "Data columns (total 32 columns):\n",
      " #   Column                   Non-Null Count  Dtype  \n",
      "---  ------                   --------------  -----  \n",
      " 0   id                       569 non-null    int64  \n",
      " 1   diagnosis                569 non-null    object \n",
      " 2   radius_mean              569 non-null    float64\n",
      " 3   texture_mean             569 non-null    float64\n",
      " 4   perimeter_mean           569 non-null    float64\n",
      " 5   area_mean                569 non-null    float64\n",
      " 6   smoothness_mean          569 non-null    float64\n",
      " 7   compactness_mean         569 non-null    float64\n",
      " 8   concavity_mean           569 non-null    float64\n",
      " 9   concave points_mean      569 non-null    float64\n",
      " 10  symmetry_mean            569 non-null    float64\n",
      " 11  fractal_dimension_mean   569 non-null    float64\n",
      " 12  radius_se                569 non-null    float64\n",
      " 13  texture_se               569 non-null    float64\n",
      " 14  perimeter_se             569 non-null    float64\n",
      " 15  area_se                  569 non-null    float64\n",
      " 16  smoothness_se            569 non-null    float64\n",
      " 17  compactness_se           569 non-null    float64\n",
      " 18  concavity_se             569 non-null    float64\n",
      " 19  concave points_se        569 non-null    float64\n",
      " 20  symmetry_se              569 non-null    float64\n",
      " 21  fractal_dimension_se     569 non-null    float64\n",
      " 22  radius_worst             569 non-null    float64\n",
      " 23  texture_worst            569 non-null    float64\n",
      " 24  perimeter_worst          569 non-null    float64\n",
      " 25  area_worst               569 non-null    float64\n",
      " 26  smoothness_worst         569 non-null    float64\n",
      " 27  compactness_worst        569 non-null    float64\n",
      " 28  concavity_worst          569 non-null    float64\n",
      " 29  concave points_worst     569 non-null    float64\n",
      " 30  symmetry_worst           569 non-null    float64\n",
      " 31  fractal_dimension_worst  569 non-null    float64\n",
      "dtypes: float64(30), int64(1), object(1)\n",
      "memory usage: 142.4+ KB\n",
      "id                         0\n",
      "diagnosis                  0\n",
      "radius_mean                0\n",
      "texture_mean               0\n",
      "perimeter_mean             0\n",
      "area_mean                  0\n",
      "smoothness_mean            0\n",
      "compactness_mean           0\n",
      "concavity_mean             0\n",
      "concave points_mean        0\n",
      "symmetry_mean              0\n",
      "fractal_dimension_mean     0\n",
      "radius_se                  0\n",
      "texture_se                 0\n",
      "perimeter_se               0\n",
      "area_se                    0\n",
      "smoothness_se              0\n",
      "compactness_se             0\n",
      "concavity_se               0\n",
      "concave points_se          0\n",
      "symmetry_se                0\n",
      "fractal_dimension_se       0\n",
      "radius_worst               0\n",
      "texture_worst              0\n",
      "perimeter_worst            0\n",
      "area_worst                 0\n",
      "smoothness_worst           0\n",
      "compactness_worst          0\n",
      "concavity_worst            0\n",
      "concave points_worst       0\n",
      "symmetry_worst             0\n",
      "fractal_dimension_worst    0\n",
      "dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>diagnosis</th>\n",
       "      <th>radius_mean</th>\n",
       "      <th>texture_mean</th>\n",
       "      <th>perimeter_mean</th>\n",
       "      <th>area_mean</th>\n",
       "      <th>smoothness_mean</th>\n",
       "      <th>compactness_mean</th>\n",
       "      <th>concavity_mean</th>\n",
       "      <th>concave points_mean</th>\n",
       "      <th>...</th>\n",
       "      <th>radius_worst</th>\n",
       "      <th>texture_worst</th>\n",
       "      <th>perimeter_worst</th>\n",
       "      <th>area_worst</th>\n",
       "      <th>smoothness_worst</th>\n",
       "      <th>compactness_worst</th>\n",
       "      <th>concavity_worst</th>\n",
       "      <th>concave points_worst</th>\n",
       "      <th>symmetry_worst</th>\n",
       "      <th>fractal_dimension_worst</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>842302</td>\n",
       "      <td>M</td>\n",
       "      <td>17.99</td>\n",
       "      <td>10.38</td>\n",
       "      <td>122.80</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>0.11840</td>\n",
       "      <td>0.27760</td>\n",
       "      <td>0.3001</td>\n",
       "      <td>0.14710</td>\n",
       "      <td>...</td>\n",
       "      <td>25.38</td>\n",
       "      <td>17.33</td>\n",
       "      <td>184.60</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>0.1622</td>\n",
       "      <td>0.6656</td>\n",
       "      <td>0.7119</td>\n",
       "      <td>0.2654</td>\n",
       "      <td>0.4601</td>\n",
       "      <td>0.11890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>842517</td>\n",
       "      <td>M</td>\n",
       "      <td>20.57</td>\n",
       "      <td>17.77</td>\n",
       "      <td>132.90</td>\n",
       "      <td>1326.0</td>\n",
       "      <td>0.08474</td>\n",
       "      <td>0.07864</td>\n",
       "      <td>0.0869</td>\n",
       "      <td>0.07017</td>\n",
       "      <td>...</td>\n",
       "      <td>24.99</td>\n",
       "      <td>23.41</td>\n",
       "      <td>158.80</td>\n",
       "      <td>1956.0</td>\n",
       "      <td>0.1238</td>\n",
       "      <td>0.1866</td>\n",
       "      <td>0.2416</td>\n",
       "      <td>0.1860</td>\n",
       "      <td>0.2750</td>\n",
       "      <td>0.08902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>84300903</td>\n",
       "      <td>M</td>\n",
       "      <td>19.69</td>\n",
       "      <td>21.25</td>\n",
       "      <td>130.00</td>\n",
       "      <td>1203.0</td>\n",
       "      <td>0.10960</td>\n",
       "      <td>0.15990</td>\n",
       "      <td>0.1974</td>\n",
       "      <td>0.12790</td>\n",
       "      <td>...</td>\n",
       "      <td>23.57</td>\n",
       "      <td>25.53</td>\n",
       "      <td>152.50</td>\n",
       "      <td>1709.0</td>\n",
       "      <td>0.1444</td>\n",
       "      <td>0.4245</td>\n",
       "      <td>0.4504</td>\n",
       "      <td>0.2430</td>\n",
       "      <td>0.3613</td>\n",
       "      <td>0.08758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>84348301</td>\n",
       "      <td>M</td>\n",
       "      <td>11.42</td>\n",
       "      <td>20.38</td>\n",
       "      <td>77.58</td>\n",
       "      <td>386.1</td>\n",
       "      <td>0.14250</td>\n",
       "      <td>0.28390</td>\n",
       "      <td>0.2414</td>\n",
       "      <td>0.10520</td>\n",
       "      <td>...</td>\n",
       "      <td>14.91</td>\n",
       "      <td>26.50</td>\n",
       "      <td>98.87</td>\n",
       "      <td>567.7</td>\n",
       "      <td>0.2098</td>\n",
       "      <td>0.8663</td>\n",
       "      <td>0.6869</td>\n",
       "      <td>0.2575</td>\n",
       "      <td>0.6638</td>\n",
       "      <td>0.17300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>84358402</td>\n",
       "      <td>M</td>\n",
       "      <td>20.29</td>\n",
       "      <td>14.34</td>\n",
       "      <td>135.10</td>\n",
       "      <td>1297.0</td>\n",
       "      <td>0.10030</td>\n",
       "      <td>0.13280</td>\n",
       "      <td>0.1980</td>\n",
       "      <td>0.10430</td>\n",
       "      <td>...</td>\n",
       "      <td>22.54</td>\n",
       "      <td>16.67</td>\n",
       "      <td>152.20</td>\n",
       "      <td>1575.0</td>\n",
       "      <td>0.1374</td>\n",
       "      <td>0.2050</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.1625</td>\n",
       "      <td>0.2364</td>\n",
       "      <td>0.07678</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows √ó 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         id diagnosis  radius_mean  texture_mean  perimeter_mean  area_mean  \\\n",
       "0    842302         M        17.99         10.38          122.80     1001.0   \n",
       "1    842517         M        20.57         17.77          132.90     1326.0   \n",
       "2  84300903         M        19.69         21.25          130.00     1203.0   \n",
       "3  84348301         M        11.42         20.38           77.58      386.1   \n",
       "4  84358402         M        20.29         14.34          135.10     1297.0   \n",
       "\n",
       "   smoothness_mean  compactness_mean  concavity_mean  concave points_mean  \\\n",
       "0          0.11840           0.27760          0.3001              0.14710   \n",
       "1          0.08474           0.07864          0.0869              0.07017   \n",
       "2          0.10960           0.15990          0.1974              0.12790   \n",
       "3          0.14250           0.28390          0.2414              0.10520   \n",
       "4          0.10030           0.13280          0.1980              0.10430   \n",
       "\n",
       "   ...  radius_worst  texture_worst  perimeter_worst  area_worst  \\\n",
       "0  ...         25.38          17.33           184.60      2019.0   \n",
       "1  ...         24.99          23.41           158.80      1956.0   \n",
       "2  ...         23.57          25.53           152.50      1709.0   \n",
       "3  ...         14.91          26.50            98.87       567.7   \n",
       "4  ...         22.54          16.67           152.20      1575.0   \n",
       "\n",
       "   smoothness_worst  compactness_worst  concavity_worst  concave points_worst  \\\n",
       "0            0.1622             0.6656           0.7119                0.2654   \n",
       "1            0.1238             0.1866           0.2416                0.1860   \n",
       "2            0.1444             0.4245           0.4504                0.2430   \n",
       "3            0.2098             0.8663           0.6869                0.2575   \n",
       "4            0.1374             0.2050           0.4000                0.1625   \n",
       "\n",
       "   symmetry_worst  fractal_dimension_worst  \n",
       "0          0.4601                  0.11890  \n",
       "1          0.2750                  0.08902  \n",
       "2          0.3613                  0.08758  \n",
       "3          0.6638                  0.17300  \n",
       "4          0.2364                  0.07678  \n",
       "\n",
       "[5 rows x 32 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Step 0: Import required libraries\n",
    "# Load and Explore the Data\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "# 1. Load the digits dataset breast-cancer-data.csv\n",
    "dataset=pd.read_csv('breast-cancer-data.csv')\n",
    "dataset.info()\n",
    "print(dataset.isnull().sum())\n",
    "dataset.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d2882eae-1b40-4bcd-83b5-c6b336ffcb9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Step 1: \n",
    "# Dropping irrelevant columns like id.\n",
    "# Encoding categorical labels (diagnosis) into numeric form.\n",
    "# Standardizing the data for dimensionality reduction.\n",
    "\n",
    "# Drop 'id' column\n",
    "dataset = dataset.drop(columns=['id'])\n",
    "\n",
    "# Encode 'diagnosis' column (M -> 1, B -> 0)\n",
    "dataset['diagnosis'] = dataset['diagnosis'].map({'M': 1, 'B': 0})\n",
    "\n",
    "# Separate features and target\n",
    "X = dataset.drop(columns=['diagnosis'])\n",
    "y = dataset['diagnosis']\n",
    "\n",
    "# Standardize the features\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_standardized = scaler.fit_transform(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7fdeb2db-d977-44ee-b469-8639037e5ec3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of components retained by PCA: 10\n",
      "LDA reduced the data to 1 component(s).\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Apply Dimensionality Reduction (PCA & LDA)\n",
    "# PCA:\n",
    "# Use PCA to reduce dimensions while retaining a high percentage of variance (e.g., 95%).\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Perform PCA\n",
    "pca = PCA(n_components=0.95)\n",
    "X_pca = pca.fit_transform(X_standardized)\n",
    "\n",
    "# Check the number of components retained\n",
    "num_components_pca = pca.n_components_\n",
    "print(f\"Number of components retained by PCA: {num_components_pca}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "\n",
    "# Perform LDA\n",
    "lda = LDA(n_components=1)  # Since it's a binary classification problem\n",
    "X_lda = lda.fit_transform(X_standardized, y)\n",
    "print(f\"LDA reduced the data to {X_lda.shape[1]} component(s).\")\n",
    "\n",
    "# LDA is a supervised dimensionality reduction technique, and the maximum number of components (k) is determined by the number of classes in the dataset. \n",
    "# The formula is:\n",
    "# ùëòmax = ùê∂ ‚àí 1\n",
    "# Where C is the number of unique classes. \n",
    "# Since we are working with a binary classification problem (classes: Malignant and Benign, C=2), the maximum k is 1.\n",
    "# This is why we reduced the data to 1 component in LDA.\n",
    "# LDA ensures that this single component contains the most discriminative information for separating the two classes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5d2bb96a-d4e2-4507-b26f-29b737b580f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline Model Accuracy: 97.37%\n",
      "PCA Model Accuracy: 98.25%\n",
      "LDA Model Accuracy: 97.37%\n"
     ]
    }
   ],
   "source": [
    "# Step 3: Compare Models with and Without Dimensionality Reduction\n",
    "# Train and evaluate logistic regression models:\n",
    "\n",
    "# Without dimensionality reduction.\n",
    "# After PCA.\n",
    "# After LDA.\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "\n",
    "# Split the data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_standardized, y, test_size=0.2, random_state=42)\n",
    "X_train_pca, X_test_pca = train_test_split(X_pca, test_size=0.2, random_state=42)\n",
    "X_train_lda, X_test_lda = train_test_split(X_lda, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize Logistic Regression\n",
    "logistic_model = LogisticRegression(max_iter=10000)\n",
    "\n",
    "# Baseline Model (No Dimensionality Reduction)\n",
    "logistic_model.fit(X_train, y_train)\n",
    "y_pred_baseline = logistic_model.predict(X_test)\n",
    "accuracy_baseline = accuracy_score(y_test, y_pred_baseline)\n",
    "print(f\"Baseline Model Accuracy: {accuracy_baseline * 100:.2f}%\")\n",
    "\n",
    "# PCA Model\n",
    "logistic_model.fit(X_train_pca, y_train)\n",
    "y_pred_pca = logistic_model.predict(X_test_pca)\n",
    "accuracy_pca = accuracy_score(y_test, y_pred_pca)\n",
    "print(f\"PCA Model Accuracy: {accuracy_pca * 100:.2f}%\")\n",
    "\n",
    "# LDA Model\n",
    "logistic_model.fit(X_train_lda, y_train)\n",
    "y_pred_lda = logistic_model.predict(X_test_lda)\n",
    "accuracy_lda = accuracy_score(y_test, y_pred_lda)\n",
    "print(f\"LDA Model Accuracy: {accuracy_lda * 100:.2f}%\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2e74bdb8-9ebb-43b8-a058-841f34c60820",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Baseline Model Evaluation:\n",
      "Confusion Matrix:\n",
      "[[70  1]\n",
      " [ 2 41]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.99      0.98        71\n",
      "           1       0.98      0.95      0.96        43\n",
      "\n",
      "    accuracy                           0.97       114\n",
      "   macro avg       0.97      0.97      0.97       114\n",
      "weighted avg       0.97      0.97      0.97       114\n",
      "\n",
      "\n",
      "PCA Model Evaluation:\n",
      "Confusion Matrix:\n",
      "[[70  1]\n",
      " [ 1 42]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.99      0.99        71\n",
      "           1       0.98      0.98      0.98        43\n",
      "\n",
      "    accuracy                           0.98       114\n",
      "   macro avg       0.98      0.98      0.98       114\n",
      "weighted avg       0.98      0.98      0.98       114\n",
      "\n",
      "\n",
      "LDA Model Evaluation:\n",
      "Confusion Matrix:\n",
      "[[70  1]\n",
      " [ 2 41]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.99      0.98        71\n",
      "           1       0.98      0.95      0.96        43\n",
      "\n",
      "    accuracy                           0.97       114\n",
      "   macro avg       0.97      0.97      0.97       114\n",
      "weighted avg       0.97      0.97      0.97       114\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Step 4: Evaluate Model Performance\n",
    "# Evaluate performance for each model using confusion matrix, precision, recall, and F1-score.\n",
    "\n",
    "# Function to evaluate model\n",
    "def evaluate_model(y_true, y_pred, model_name):\n",
    "    print(f\"\\n{model_name} Model Evaluation:\")\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(confusion_matrix(y_true, y_pred))\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y_true, y_pred))\n",
    "\n",
    "# Evaluate each model\n",
    "evaluate_model(y_test, y_pred_baseline, \"Baseline\")\n",
    "evaluate_model(y_test, y_pred_pca, \"PCA\")\n",
    "evaluate_model(y_test, y_pred_lda, \"LDA\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1fa48c5-6d2c-4523-bc3c-9e66616c3b21",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "76741985-fa1c-423e-b59e-be442d75b846",
   "metadata": {},
   "source": [
    "Summary and Conclusion\n",
    "Based on the results:\n",
    "\n",
    "1. Baseline Model\n",
    "Accuracy: 97.37%\n",
    "Confusion Matrix:\n",
    "True Negatives: 70\n",
    "False Positives: 1\n",
    "False Negatives: 2\n",
    "True Positives: 41\n",
    "Precision, Recall, and F1-Score:\n",
    "Class 0 (No Cancer): Precision: 97%, Recall: 99%, F1-Score: 98%\n",
    "Class 1 (Cancer): Precision: 98%, Recall: 95%, F1-Score: 96%\n",
    "Observations:\n",
    "High accuracy but uses all 30 features, making the model complex and less interpretable.\n",
    "\n",
    "\n",
    "2. PCA Model\n",
    "Number of Components Retained: 10\n",
    "Accuracy: 98.25%\n",
    "Confusion Matrix:\n",
    "True Negatives: 70\n",
    "False Positives: 1\n",
    "False Negatives: 1\n",
    "True Positives: 42\n",
    "Precision, Recall, and F1-Score:\n",
    "Class 0 (No Cancer): Precision: 99%, Recall: 99%, F1-Score: 99%\n",
    "Class 1 (Cancer): Precision: 98%, Recall: 98%, F1-Score: 98%\n",
    "Observations:\n",
    "PCA achieved the highest accuracy with only 10 components, significantly reducing dimensionality while improving accuracy.\n",
    "Superior precision and recall for both classes compared to the baseline.\n",
    "\n",
    "   \n",
    "3. LDA Model\n",
    "Number of Components Retained: 1\n",
    "Accuracy: 97.37%\n",
    "Confusion Matrix:\n",
    "True Negatives: 70\n",
    "False Positives: 1\n",
    "False Negatives: 2\n",
    "True Positives: 41\n",
    "Precision, Recall, and F1-Score:\n",
    "Class 0 (No Cancer): Precision: 97%, Recall: 99%, F1-Score: 98%\n",
    "Class 1 (Cancer): Precision: 98%, Recall: 95%, F1-Score: 96%\n",
    "Observations:\n",
    "LDA simplifies the model to a single component, achieving the same accuracy as the baseline.\n",
    "Excellent interpretability and reduced complexity make it ideal for communicating results to doctors.\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "caaacdef-1f86-465b-8f73-7fcafc6b2f65",
   "metadata": {},
   "source": [
    "Final Recommendation\n",
    "PCA: The best performing model with an accuracy of 98.25% while reducing dimensions to 10. It balances accuracy and dimensionality reduction effectively.\n",
    "LDA: While its accuracy matches the baseline, the simplicity of reducing to just 1 component and its focus on class separability make it a highly interpretable choice for classification tasks.\n",
    "Baseline: High accuracy, but the complexity of using all 30 features makes it less practical for deployment or explanation.\n",
    "\n",
    "Conclusion:\n",
    "Use PCA for the best accuracy with reduced dimensions.\n",
    "Consider LDA for situations where simplicity and interpretability are the primary goals."
   ]
  },
  {
   "cell_type": "raw",
   "id": "98493030-627d-4def-a681-858b37671017",
   "metadata": {},
   "source": [
    "PCA and LDA:\n",
    "Designed primarily for dimensionality reduction.\n",
    "PCA is versatile (works for both regression and classification).\n",
    "LDA is tailored for classification, especially when interpretability matters.\n",
    "\n",
    "DT and RF:\n",
    "Naturally handle feature selection, so dimensionality reduction is often not required.\n",
    "Work well on raw datasets, even with irrelevant features.\n",
    "\n",
    "Practical Usage:\n",
    "Use PCA or LDA for reducing dimensions before applying regression models or simpler classification algorithms like logistic regression.\n",
    "Use DT or RF directly on raw data unless dimensionality is extremely high, in which case PCA or LDA can still aid preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fe07460-13a6-4b88-8f97-62e5ca002619",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f7a3f47-b774-4b02-a73b-011eb2ddafdd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
