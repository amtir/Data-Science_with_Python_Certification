{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "252d37d6-20f0-46a4-86ab-61e0a94362f1",
   "metadata": {},
   "source": [
    "# Module 8: Dimensionality Reduction\n",
    "## Case Study â€“ 2\n",
    "\n",
    "### Understand and practice linear discriminant analysis using scikit learn. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0b5a9667-c283-4de8-8a6d-c2fc769db208",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".. _digits_dataset:\n",
      "\n",
      "Optical recognition of handwritten digits dataset\n",
      "--------------------------------------------------\n",
      "\n",
      "**Data Set Characteristics:**\n",
      "\n",
      ":Number of Instances: 1797\n",
      ":Number of Attributes: 64\n",
      ":Attribute Information: 8x8 image of integer pixels in the range 0..16.\n",
      ":Missing Attribute Values: None\n",
      ":Creator: E. Alpaydin (alpaydin '@' boun.edu.tr)\n",
      ":Date: July; 1998\n",
      "\n",
      "This is a copy of the test set of the UCI ML hand-written digits datasets\n",
      "https://archive.ics.uci.edu/ml/datasets/Optical+Recognition+of+Handwritten+Digits\n",
      "\n",
      "The data set contains images of hand-written digits: 10 classes where\n",
      "each class refers to a digit.\n",
      "\n",
      "Preprocessing programs made available by NIST were used to extract\n",
      "normalized bitmaps of handwritten digits from a preprinted form. From a\n",
      "total of 43 people, 30 contributed to the training set and different 13\n",
      "to the test set. 32x32 bitmaps are divided into nonoverlapping blocks of\n",
      "4x4 and the number of on pixels are counted in each block. This generates\n",
      "an input matrix of 8x8 where each element is an integer in the range\n",
      "0..16. This reduces dimensionality and gives invariance to small\n",
      "distortions.\n",
      "\n",
      "For info on NIST preprocessing routines, see M. D. Garris, J. L. Blue, G.\n",
      "T. Candela, D. L. Dimmick, J. Geist, P. J. Grother, S. A. Janet, and C.\n",
      "L. Wilson, NIST Form-Based Handprint Recognition System, NISTIR 5469,\n",
      "1994.\n",
      "\n",
      ".. dropdown:: References\n",
      "\n",
      "  - C. Kaynak (1995) Methods of Combining Multiple Classifiers and Their\n",
      "    Applications to Handwritten Digit Recognition, MSc Thesis, Institute of\n",
      "    Graduate Studies in Science and Engineering, Bogazici University.\n",
      "  - E. Alpaydin, C. Kaynak (1998) Cascading Classifiers, Kybernetika.\n",
      "  - Ken Tang and Ponnuthurai N. Suganthan and Xi Yao and A. Kai Qin.\n",
      "    Linear dimensionalityreduction using relevance weighted LDA. School of\n",
      "    Electrical and Electronic Engineering Nanyang Technological University.\n",
      "    2005.\n",
      "  - Claudio Gentile. A New Approximate Maximal Margin Classification\n",
      "    Algorithm. NIPS. 2000.\n",
      "\n",
      "[0]\n",
      "[[[ 0.  0.  5. 13.  9.  1.  0.  0.]\n",
      "  [ 0.  0. 13. 15. 10. 15.  5.  0.]\n",
      "  [ 0.  3. 15.  2.  0. 11.  8.  0.]\n",
      "  [ 0.  4. 12.  0.  0.  8.  8.  0.]\n",
      "  [ 0.  5.  8.  0.  0.  9.  8.  0.]\n",
      "  [ 0.  4. 11.  0.  1. 12.  7.  0.]\n",
      "  [ 0.  2. 14.  5. 10. 12.  0.  0.]\n",
      "  [ 0.  0.  6. 13. 10.  0.  0.  0.]]]\n"
     ]
    }
   ],
   "source": [
    "# 1. Scikit learn comes with the pre-loaded dataset, load the digits dataset from \n",
    "# that collection and write a helper function to plot the image using matplotlib. \n",
    "# [Hint: Explore datasets module from scikit learn] \n",
    "\n",
    "# Step 1: Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# 1.\n",
    "# Step 2: Load the digits dataset\n",
    "from sklearn.datasets import load_digits\n",
    "digits = load_digits()\n",
    "print(digits.DESCR)\n",
    "print(digits.target_names[[0]])\n",
    "print(digits.images[[0]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4acb438c-19e9-4566-b769-a41de444dd4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Helper function to plot an image\n",
    "def plot_digit_image(index):\n",
    "    # Select the image at the given index\n",
    "    image = digits.images[index]\n",
    "    label = digits.target[index]\n",
    "    \n",
    "    # Plot the image with matplotlib\n",
    "    plt.imshow(image, cmap='gray')\n",
    "    plt.title(f\"Digit: {label}\")\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "# Test the function\n",
    "plot_digit_image(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "af84ed66-6605-4372-8365-5651ffddedeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. StandardScaler and Perform an 80-20 Split\n",
    "# We will use the digits dataset from scikit-learn and perform an 80-20 train-test split.\n",
    "\n",
    "X = digits.data\n",
    "y = digits.target\n",
    "\n",
    "# Standardize the data\n",
    "scaler = StandardScaler()\n",
    "X_standardized = scaler.fit_transform(X)\n",
    "\n",
    "# Split the data into train and test sets (80-20 split)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_standardized, y, test_size=0.2, random_state=42)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "20f8d3a9-5336-40e6-8cd3-15d0659c2a6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Logistic Regression without PCA/LDA: 97.22%\n",
      "Confusion Matrix for Baseline Logistic Regression:\n",
      "[[33  0  0  0  0  0  0  0  0  0]\n",
      " [ 0 28  0  0  0  0  0  0  0  0]\n",
      " [ 0  0 33  0  0  0  0  0  0  0]\n",
      " [ 0  0  0 33  0  1  0  0  0  0]\n",
      " [ 0  1  0  0 45  0  0  0  0  0]\n",
      " [ 0  0  0  0  0 44  1  0  0  2]\n",
      " [ 0  0  0  0  0  1 34  0  0  0]\n",
      " [ 0  0  0  0  0  0  0 33  0  1]\n",
      " [ 0  0  0  0  0  1  0  0 29  0]\n",
      " [ 0  0  0  1  0  0  0  0  1 38]]\n",
      "\n",
      "Classification Report for Baseline Logistic Regression:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        33\n",
      "           1       0.97      1.00      0.98        28\n",
      "           2       1.00      1.00      1.00        33\n",
      "           3       0.97      0.97      0.97        34\n",
      "           4       1.00      0.98      0.99        46\n",
      "           5       0.94      0.94      0.94        47\n",
      "           6       0.97      0.97      0.97        35\n",
      "           7       1.00      0.97      0.99        34\n",
      "           8       0.97      0.97      0.97        30\n",
      "           9       0.93      0.95      0.94        40\n",
      "\n",
      "    accuracy                           0.97       360\n",
      "   macro avg       0.97      0.97      0.97       360\n",
      "weighted avg       0.97      0.97      0.97       360\n",
      "\n",
      "Cross-validation Accuracy for Baseline Logistic Regression: 96.17%\n"
     ]
    }
   ],
   "source": [
    "# 3. Logistic Regression Without PCA or LDA\n",
    "# A a baseline model with logistic regression applied directly to the standardized data without PCA or LDA \n",
    "# is an excellent way to evaluate potential overfitting and establish a comparison point for PCA and LDA. \n",
    "# \n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "\n",
    "# Fit Logistic Regression directly on standardized data\n",
    "logistic_model_baseline = LogisticRegression(max_iter=10000, solver='lbfgs', multi_class='multinomial')\n",
    "logistic_model_baseline.fit(X_train, y_train)\n",
    "\n",
    "# Predict and evaluate\n",
    "y_pred_baseline = logistic_model_baseline.predict(X_test)\n",
    "accuracy_baseline = accuracy_score(y_test, y_pred_baseline)\n",
    "\n",
    "print(f\"Accuracy of Logistic Regression without PCA/LDA: {accuracy_baseline * 100:.2f}%\")\n",
    "print(\"Confusion Matrix for Baseline Logistic Regression:\")\n",
    "print(confusion_matrix(y_test, y_pred_baseline))\n",
    "print(\"\\nClassification Report for Baseline Logistic Regression:\")\n",
    "print(classification_report(y_test, y_pred_baseline))\n",
    "\n",
    "# Cross-validation\n",
    "cv_scores_baseline = cross_val_score(logistic_model_baseline, X_train, y_train, cv=5, scoring='accuracy')\n",
    "print(f\"Cross-validation Accuracy for Baseline Logistic Regression: {cv_scores_baseline.mean() * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2434e8ca-2d32-41f5-b4e0-3eb3e9106127",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Logistic Regression after PCA: 96.39%\n",
      "Confusion Matrix for PCA:\n",
      "[[33  0  0  0  0  0  0  0  0  0]\n",
      " [ 0 27  1  0  0  0  0  0  0  0]\n",
      " [ 0  0 32  0  0  0  0  0  1  0]\n",
      " [ 0  0  0 33  0  1  0  0  0  0]\n",
      " [ 0  0  0  0 46  0  0  0  0  0]\n",
      " [ 0  0  0  0  0 44  1  0  0  2]\n",
      " [ 0  0  0  0  0  1 34  0  0  0]\n",
      " [ 0  0  0  0  0  1  0 33  0  0]\n",
      " [ 0  1  0  0  0  1  0  0 28  0]\n",
      " [ 0  0  0  1  0  0  0  0  2 37]]\n",
      "\n",
      "Classification Report for PCA:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        33\n",
      "           1       0.96      0.96      0.96        28\n",
      "           2       0.97      0.97      0.97        33\n",
      "           3       0.97      0.97      0.97        34\n",
      "           4       1.00      1.00      1.00        46\n",
      "           5       0.92      0.94      0.93        47\n",
      "           6       0.97      0.97      0.97        35\n",
      "           7       1.00      0.97      0.99        34\n",
      "           8       0.90      0.93      0.92        30\n",
      "           9       0.95      0.93      0.94        40\n",
      "\n",
      "    accuracy                           0.96       360\n",
      "   macro avg       0.96      0.96      0.96       360\n",
      "weighted avg       0.96      0.96      0.96       360\n",
      "\n",
      "Cross-validation Accuracy for PCA: 95.27%\n",
      "Number of components retained by PCA to explain 95% variance: 40\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 4. Perform PCA and Train Logistic Regression\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# PCA to retain 95% variance\n",
    "pca = PCA(n_components=0.95)\n",
    "X_train_pca = pca.fit_transform(X_train)\n",
    "X_test_pca = pca.transform(X_test)\n",
    "\n",
    "# Fit Logistic Regression\n",
    "logistic_model_pca = LogisticRegression(max_iter=10000, solver='lbfgs', multi_class='multinomial')\n",
    "logistic_model_pca.fit(X_train_pca, y_train)\n",
    "\n",
    "# Predict and evaluate\n",
    "y_pred_pca = logistic_model_pca.predict(X_test_pca)\n",
    "accuracy_pca = accuracy_score(y_test, y_pred_pca)\n",
    "\n",
    "print(f\"Accuracy of Logistic Regression after PCA: {accuracy_pca * 100:.2f}%\")\n",
    "print(\"Confusion Matrix for PCA:\")\n",
    "print(confusion_matrix(y_test, y_pred_pca))\n",
    "print(\"\\nClassification Report for PCA:\")\n",
    "print(classification_report(y_test, y_pred_pca))\n",
    "\n",
    "# Cross-validation\n",
    "cv_scores_pca = cross_val_score(logistic_model_pca, X_train_pca, y_train, cv=5, scoring='accuracy')\n",
    "print(f\"Cross-validation Accuracy for PCA: {cv_scores_pca.mean() * 100:.2f}%\")\n",
    "\n",
    "# Retrieve the number of components retained by PCA\n",
    "num_components_pca = pca.n_components_\n",
    "print(f\"Number of components retained by PCA to explain 95% variance: {num_components_pca}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1e454b74-eae6-4142-9464-2b3c55eb81c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Logistic Regression after LDA: 95.00%\n",
      "Confusion Matrix for LDA:\n",
      "[[32  0  0  0  0  0  0  1  0  0]\n",
      " [ 0 27  1  0  0  0  0  0  0  0]\n",
      " [ 0  0 32  1  0  0  0  0  0  0]\n",
      " [ 0  0  0 33  0  1  0  0  0  0]\n",
      " [ 0  0  0  0 45  0  1  0  0  0]\n",
      " [ 0  0  1  0  0 44  1  0  0  1]\n",
      " [ 1  0  0  0  0  0 34  0  0  0]\n",
      " [ 0  0  0  0  0  0  0 33  0  1]\n",
      " [ 0  2  0  0  0  1  0  0 27  0]\n",
      " [ 0  1  0  1  0  1  0  0  2 35]]\n",
      "\n",
      "Classification Report for LDA:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.97      0.97        33\n",
      "           1       0.90      0.96      0.93        28\n",
      "           2       0.94      0.97      0.96        33\n",
      "           3       0.94      0.97      0.96        34\n",
      "           4       1.00      0.98      0.99        46\n",
      "           5       0.94      0.94      0.94        47\n",
      "           6       0.94      0.97      0.96        35\n",
      "           7       0.97      0.97      0.97        34\n",
      "           8       0.93      0.90      0.92        30\n",
      "           9       0.95      0.88      0.91        40\n",
      "\n",
      "    accuracy                           0.95       360\n",
      "   macro avg       0.95      0.95      0.95       360\n",
      "weighted avg       0.95      0.95      0.95       360\n",
      "\n",
      "Cross-validation Accuracy for LDA: 96.38%\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 5. Perform LDA and Train Logistic Regression\n",
    "\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "\n",
    "# LDA with supervised labels\n",
    "lda = LinearDiscriminantAnalysis()\n",
    "X_train_lda = lda.fit_transform(X_train, y_train)\n",
    "X_test_lda = lda.transform(X_test)\n",
    "\n",
    "# Fit Logistic Regression\n",
    "logistic_model_lda = LogisticRegression(max_iter=10000, solver='lbfgs', multi_class='multinomial')\n",
    "logistic_model_lda.fit(X_train_lda, y_train)\n",
    "\n",
    "# Predict and evaluate\n",
    "y_pred_lda = logistic_model_lda.predict(X_test_lda)\n",
    "accuracy_lda = accuracy_score(y_test, y_pred_lda)\n",
    "\n",
    "print(f\"Accuracy of Logistic Regression after LDA: {accuracy_lda * 100:.2f}%\")\n",
    "print(\"Confusion Matrix for LDA:\")\n",
    "print(confusion_matrix(y_test, y_pred_lda))\n",
    "print(\"\\nClassification Report for LDA:\")\n",
    "print(classification_report(y_test, y_pred_lda))\n",
    "\n",
    "# Cross-validation\n",
    "cv_scores_lda = cross_val_score(logistic_model_lda, X_train_lda, y_train, cv=5, scoring='accuracy')\n",
    "print(f\"Cross-validation Accuracy for LDA: {cv_scores_lda.mean() * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d1a6fde6-42ee-4434-9f65-bf528eb380bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Performance Comparison:\n",
      "Baseline - Accuracy: 97.22%, Cross-Validation Accuracy: 96.17%\n",
      "PCA - Accuracy: 96.39%, Cross-Validation Accuracy: 95.27%\n",
      "LDA - Accuracy: 95.00%, Cross-Validation Accuracy: 96.38%\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 6. Compare Performance\n",
    "\n",
    "print(\"\\nPerformance Comparison:\")\n",
    "print(f\"Baseline - Accuracy: {accuracy_baseline * 100:.2f}%, Cross-Validation Accuracy: {cv_scores_baseline.mean() * 100:.2f}%\")\n",
    "print(f\"PCA - Accuracy: {accuracy_pca * 100:.2f}%, Cross-Validation Accuracy: {cv_scores_pca.mean() * 100:.2f}%\")\n",
    "print(f\"LDA - Accuracy: {accuracy_lda * 100:.2f}%, Cross-Validation Accuracy: {cv_scores_lda.mean() * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ab5fbfda-8cb1-46bb-8e1b-320a6f5e2781",
   "metadata": {},
   "source": [
    "7. Final Summary and Conclusion\n",
    "Based on the performance metrics and dimensionality reduction details:\n",
    "\n",
    "I. Baseline Logistic Regression (No Dimensionality Reduction):\n",
    "Test Accuracy: 97.22%\n",
    "Cross-Validation Accuracy: 96.17%\n",
    "Dimensions: 64 (no reduction)\n",
    "Observations:\n",
    "Highest test accuracy, but uses all 64 features, which increases model complexity and the potential for overfitting.\n",
    "The slight drop in cross-validation accuracy compared to the test accuracy suggests the model might slightly overfit.\n",
    "\n",
    "\n",
    "II. PCA + Logistic Regression:\n",
    "Test Accuracy: 96.39%\n",
    "Cross-Validation Accuracy: 95.27%\n",
    "Dimensions: 40 (reduced from 64, explains 95% variance)\n",
    "Observations:\n",
    "PCA reduced dimensionality by retaining only 40 components, significantly reducing model complexity.\n",
    "There is a slight drop in accuracy compared to the baseline, but this is expected due to the trade-off between simplicity and accuracy.\n",
    "PCA is an efficient unsupervised dimensionality reduction method, and it balances generalization and computational efficiency.\n",
    "\n",
    "\n",
    "III. LDA + Logistic Regression:\n",
    "Test Accuracy: 95.00%\n",
    "Cross-Validation Accuracy: 96.38%\n",
    "Dimensions: 9 (determined by number of classes - 1)\n",
    "Observations:\n",
    "LDA reduced dimensionality to just 9 components, significantly simplifying the model.\n",
    "Slightly lower test accuracy compared to PCA and the baseline but with excellent cross-validation accuracy.\n",
    "LDA explicitly considers class separability, which makes it robust, especially for smaller datasets with a clear distinction between classes.\n",
    "\n",
    "                                                                                                   "
   ]
  },
  {
   "cell_type": "raw",
   "id": "92ddd289-3d08-4994-9270-1ff7313412be",
   "metadata": {},
   "source": [
    "8. Final Word\n",
    "Given the context and results, LDA is the best candidate for this task, which aligns with our discussion that LDA is particularly suitable for classification algorithms. Its supervised nature ensures that dimensionality reduction preserves class separability, making it an excellent choice for reducing dimensions in classification problems while maintaining or improving predictive accuracy.\n",
    "\n",
    "The comparison shows that although PCA and the baseline logistic regression perform well, LDA's combination of dimensionality reduction and class separability makes it ideal for supervised tasks like this one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27f145b3-baa9-4b41-bb31-7376caba1d4d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
